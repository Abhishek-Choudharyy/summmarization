{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(781, 2)\n"
     ]
    }
   ],
   "source": [
    "# Inshorts API : https://github.com/pari08tosh/Inshorts-API\n",
    "# Collect Data and save to file\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def getNews(category):\n",
    "    newsDictionary = {\n",
    "        'success': True,\n",
    "        'category': category,\n",
    "        'data': []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        htmlBody = requests.get('https://www.inshorts.com/en/read/' + category)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        newsDictionary['success'] = False\n",
    "        newsDictionary['errorMessage'] = str(e.message)\n",
    "        return newsDictionary\n",
    "\n",
    "    soup = BeautifulSoup(htmlBody.text, 'lxml')\n",
    "    newsCards = soup.find_all(class_='news-card')\n",
    "    if not newsCards:\n",
    "        newsDictionary['success'] = False\n",
    "        newsDictionary['errorMessage'] = 'Invalid Category'\n",
    "        return newsDictionary\n",
    "\n",
    "    for card in newsCards:\n",
    "        try:\n",
    "            title = card.find(class_='news-card-title').find('a').text\n",
    "        except AttributeError:\n",
    "            title = None\n",
    "\n",
    "        try:\n",
    "            imageUrl = card.find(\n",
    "                class_='news-card-image')['style'].split(\"'\")[1]\n",
    "        except AttributeError:\n",
    "            imageUrl = None\n",
    "\n",
    "        try:\n",
    "            url = ('https://www.inshorts.com' + card.find(class_='news-card-title')\n",
    "                   .find('a').get('href'))\n",
    "        except AttributeError:\n",
    "            url = None\n",
    "\n",
    "        try:\n",
    "            content = card.find(class_='news-card-content').find('div').text\n",
    "        except AttributeError:\n",
    "            content = None\n",
    "\n",
    "        try:\n",
    "            author = card.find(class_='author').text\n",
    "        except AttributeError:\n",
    "            author = None\n",
    "\n",
    "        try:\n",
    "            date = card.find(clas='date').text\n",
    "        except AttributeError:\n",
    "            date = None\n",
    "\n",
    "        try:\n",
    "            time = card.find(class_='time').text\n",
    "        except AttributeError:\n",
    "            time = None\n",
    "\n",
    "        try:\n",
    "            readMoreUrl = card.find(class_='read-more').find('a').get('href')\n",
    "        except AttributeError:\n",
    "            readMoreUrl = None\n",
    "\n",
    "        newsObject = {\n",
    "            'title': title,\n",
    "            'imageUrl': imageUrl,\n",
    "            'url': url,\n",
    "            'content': content,\n",
    "            'author': author,\n",
    "            'date': date,\n",
    "            'time': time,\n",
    "            'readMoreUrl': readMoreUrl\n",
    "        }\n",
    "\n",
    "        newsDictionary['data'].append(newsObject)\n",
    "    return newsDictionary\n",
    "\n",
    "### Categories on Inshorts \n",
    "# '' // blank to get top news from all categories, national //Indian National News, business, sports, \n",
    "# world, politics, technology, startup, entertainment, miscellaneous, hatke // Unconventional, science, automobile\n",
    "\n",
    "result_df = pd.read_csv(\"inshorts_data.csv\")\n",
    "category_list = [\" \",\n",
    "                \"national\",\n",
    "                \"business\",\n",
    "                \"sports\",\n",
    "                \"world\",\n",
    "                \"politics\",\n",
    "                \"technology\",\n",
    "                \"startup\",\n",
    "                \"entertainment\",\n",
    "                \"miscellaneous\",\n",
    "                \"hatke\",\n",
    "                \"science\",\n",
    "                \"automobile\"]\n",
    "k = result_df.shape[0]\n",
    "title_list = list()\n",
    "title_list = result_df['title'].tolist()\n",
    "for cat in category_list:\n",
    "    result_news = getNews(cat)\n",
    "    for i in range(len(result_news['data'])):\n",
    "        title = result_news['data'][i]['title']\n",
    "        if title not in title_list:\n",
    "            result_df.loc[k,'title'] = title\n",
    "            result_df.loc[k,'content'] = result_news['data'][i]['content']\n",
    "            k = k + 1\n",
    "            title_list.append(title)\n",
    "print(result_df.shape)\n",
    "result_df.to_csv(\"inshorts_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning\n",
    "## https://medium.com/@acrosson/summarize-documents-using-tf-idf-bdee8f60b71\n",
    "## https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8\n",
    "\n",
    "def clean_document(document):\n",
    "    \"\"\"Cleans document by removing unnecessary punctuation. It also removes\n",
    "    any extra periods and merges acronyms to prevent the tokenizer from\n",
    "    splitting a false sentence\n",
    "    \"\"\"\n",
    "    # Remove all characters outside of Alpha Numeric\n",
    "    # and some punctuation\n",
    "    document = re.sub('[^A-Za-z .-]+', ' ', document)\n",
    "    document = document.replace('-', '')\n",
    "    document = document.replace('...', '')\n",
    "    document = document.replace('Mr.', 'Mr').replace('Mrs.', 'Mrs')\n",
    "\n",
    "    # Remove Ancronymns M.I.T. -> MIT\n",
    "    # to help with sentence tokenizing\n",
    "    document = merge_acronyms(document)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    document = ' '.join(document.split())\n",
    "    return(document)\n",
    "\n",
    "def merge_acronyms(s):\n",
    "    \"\"\"Merges all acronyms in a given sentence. For example M.I.T -> MIT\"\"\"\n",
    "    r = re.compile(r'(?:(?<=\\.|\\s)[A-Z]\\.)+')\n",
    "    acronyms = r.findall(s)\n",
    "    for a in acronyms:\n",
    "        s = s.replace(a, a.replace('.',''))\n",
    "    return(s)\n",
    "\n",
    "def remove_stop_words(document):\n",
    "    \"\"\"Returns document without stop words\"\"\"\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Pakistani prisoner,  identified as Shakir Ullah, was allegedly killed by other inmates in Rajasthan's Jaipur Central Jail following a quarrel, Jaipur Jail IG Rupinder Singh said on Wednesday. Senior police officials along with forensic experts have reached the jail, reports said. The deceased prisoner, who was lodged in the jail since 2011, was serving life sentence. Indian Super League side Bengaluru FC has offered to play an exhibition match against Real Kashmir in Srinagar to support the I-League debutants after I-League defending champions Minerva FC pulled out of their match, citing security concerns. Real Kashmir thanked Bengaluru FC and invited them in March for a friendly fixture, assuring them \"the most electric football atmosphere.\" A youth in Rajasthan's Bikaner has tattooed the names of 71 martyred soldiers, including the ones martyred in the recent Pulwama attack, on his body to pay homage to the soldiers. The youth said he hopes this will increase the respect for these soldiers amongst the people. He added that his way of paying tribute to martyrs would inspire others. Following the Pulwama terror attack, India leg-spinner Yuzvendra Chahal said that India should make things happen and teach Pakistan a lesson even if that means \"aar paar ki ladai\". \"This should settle once and for all. We can't tolerate it any longer. Every three months we get to hear about our jawans losing their lives to terrorism,\" he added.  Russia's Industry and Trade Minister Denis Manturov said on Wednesday that his country will \"definitely\" support India at the United Nations Security Council to declare Jaish-e-Mohammed (JeM) chief Masood Azhar as a global terrorist. \"(W)e would also like to convey our condolences about the (Pulwama terror) attack that happened. Russia supports India in (the) question of terrorism,\" he added. Himachal Pradesh Cricket Association (HPCA) has removed photos of Pakistani cricketers from the museum at Dharamshala stadium following the Pulwama attack. \"What our neighbour has done with us...our soldiers, it's neither acceptable now nor will ever be. No country will ever tolerate such things,\" HPCA general manager HS Manhas said. Forty CRPF jawans were martyred in the attack.  Former Jammu and Kashmir CM Mehbooba Mufti on Wednesday said that we should give proof of the Pulwama attack to Pakistan and see what they do. She added that earlier Pakistan was given proof of Pathankot and Mumbai attacks but they didn't take action. She said as Imran Khan is a new Pakistan PM, he should be given a chance.\n",
      "Former Jammu and Kashmir CM Mehbooba Mufti on Wednesday said that we should give proof of the Pulwama attack to Pakistan and see what they do.\n",
      "Following the Pulwama terror attack India legspinner Yuzvendra Chahal said that India should make things happen and teach Pakistan a lesson even if that means aar paar ki ladai .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "## Use tfidf to find score of words in sentence and then find importance sentence..\n",
    "def sent_word_frequency(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    words_dict = {}\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if(word in words_dict):\n",
    "            words_dict[word] = words_dict[word] + 1/len(words)\n",
    "        else:\n",
    "            words_dict[word] = 1/len(words)\n",
    "    return(words_dict)\n",
    "\n",
    "def create_tf_dict(document):\n",
    "    freq_list = []\n",
    "    for k in range(1,len(document)+1):\n",
    "        temp = {\"sent_id\":k,\n",
    "               \"sent_dict\":sent_word_frequency(document[k-1])}\n",
    "        freq_list.append(temp)\n",
    "    return(freq_list)\n",
    "\n",
    "def create_idf_dict(document, term_freq_list):\n",
    "    idf_list = []\n",
    "    for i in range(len(term_freq_list)):\n",
    "        for val in term_freq_list[i]['sent_dict'].keys():\n",
    "            count = sum([val in tempDict['sent_dict'] for tempDict in term_freq_list])\n",
    "            temp = {\"IDF_score\": math.log(len(document)/count), \"key\":val, \"sent_id\":i}\n",
    "            idf_list.append(temp)\n",
    "    return(idf_list)\n",
    "\n",
    "def rank_sentences(freq_list, idf_list, top_n , com_document):\n",
    "    sent_tfidf = dict()\n",
    "    for i in range(len(freq_list)):\n",
    "        temp_score = 0\n",
    "        for k,v in freq_list[i]['sent_dict'].items():\n",
    "            for data in idf_list:\n",
    "                if k == data['key']:\n",
    "                    temp_score = temp_score + v*data['IDF_score']\n",
    "\n",
    "        sent_tfidf[com_document[i]] = temp_score\n",
    "    sorted_sent_tfidf = sorted(sent_tfidf.items(), key=lambda x: -x[1])\n",
    "    sorted_list = list()\n",
    "\n",
    "    for k in range(len(sorted_sent_tfidf)):\n",
    "        sorted_list.append(sorted_sent_tfidf[k][0])\n",
    "    print(\"\\n\".join(sorted_list[:top_n]))\n",
    "    print(\"-------------\")\n",
    "    return(sorted_sent_tfidf)\n",
    "\n",
    "document = result_df.loc[0,'content'] + \" \" + result_df.loc[1,'content'] + \" \" + result_df.loc[2,'content'] + \" \" + result_df.loc[3,'content'] + \" \" + result_df.loc[4,'content'] + \" \" + result_df.loc[5,'content'] + \" \" + result_df.loc[6,'content']\n",
    "print(document)\n",
    "cleaned_document = clean_document(document)\n",
    "\n",
    "sent_text = sent_tokenize(cleaned_document)\n",
    "filtered_text =[remove_stop_words(w) for w in sent_text]\n",
    "\n",
    "summary_1 = rank_sentences(create_tf_dict(filtered_text),\n",
    "               create_idf_dict(filtered_text, create_tf_dict(filtered_text)),\n",
    "               2,\n",
    "               sent_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFIDF method but using libraby\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: pip: not found\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-b3604adad168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://github.com/pltrdy/rouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install rouge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_rouge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "# How to evaluate your summary? (Rouge(recall) and Blew(precision))\n",
    "# https://github.com/pltrdy/rouge\n",
    "!pip install rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "def find_rouge(hypothesis, reference):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
